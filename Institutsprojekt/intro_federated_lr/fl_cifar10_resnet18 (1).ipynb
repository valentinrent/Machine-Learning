{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning, Local Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Mohammad/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Libraries to be imported, please make sure you have them installed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from torchvision import datasets, transforms\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we can use the GPU to accelerate the training process. We can check if a GPU is available and set the device accordingly. This will allow us to move the data and the model to the GPU. We check if a GPU is available and set the device accordingly. We also set the random seed for reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, **kwargs):\n",
    "    # original_model = models.resnet18(weights=ResNet18_Weights.DEFAULT, **kwargs)\n",
    "    original_model = models.resnet18(**kwargs)\n",
    "\n",
    "    # Replace the first convolutional layer\n",
    "    original_model.conv1 = nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "    # Remove the max pooling layer\n",
    "    original_model.maxpool = nn.Identity()\n",
    "\n",
    "    # Replace the fully connected layer\n",
    "    original_model.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    return original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_allocation(Y, num_clients):\n",
    "    client_indices = []\n",
    "\n",
    "    # Randomly shuffle indices\n",
    "    indices = np.arange(len(Y))\n",
    "    np.random.shuffle(indices, )\n",
    "    indices_split = np.array_split(indices, num_clients)\n",
    "    client_indices = [list(idx) for idx in indices_split]\n",
    "\n",
    "    return client_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_clients = 10 # Number of clients\n",
    "batch_size = 64  # Batch size for training and testing\n",
    "global_epochs = 30  # Number of global epochs\n",
    "local_epochs = 3 # Number of local epochs (round size)\n",
    "learning_rate = 1e-2  # Learning rate for the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = resnet18(10)\n",
    "client_models = [\n",
    "    deepcopy(model).to(device) for _ in range(num_clients)\n",
    "]\n",
    "client_optims = [optim.SGD(model.parameters(), lr=learning_rate) for cm in client_models]\n",
    "\n",
    "# CIFAR-10 Dataset and Dataloaders\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(), # 1) Convert images to tensors\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # 2) Normalize the dataset\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ToDo: Download the training and test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_subsets = uniform_allocation(\n",
    "    train_dataset.targets,\n",
    "    num_clients,\n",
    ")\n",
    "train_subsets = [\n",
    "    torch.utils.data.Subset(train_dataset, indices) for indices in train_subsets\n",
    "]\n",
    "train_subset_dataloaders = [\n",
    "    DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_subsets\n",
    "]\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# ToDo: Federated Learning with FedAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Implement the FedAVG algorithm for federated learning. We do not want a version that uses a step size, i.e., a fixed number of SGD steps before averaging the models. Instead, we want a version that uses a fixed number of communication rounds. One communication round equals a local epoch on each client. We use `local_epoch` (also referred to as \"round size\") to denote the number of local epochs. \n",
    "2. After each communication round, we want to evaluate the global model on the test set. After averaging the models, we evaluate the global model on the test set. We want to store the test loss, test accuracy, and test F1 score for each communication round and plot them at the end. \n",
    "3. Use different values for `num_clients` and `local_epoch` and compare the results. What happens if you increase the number of clients? What happens if you increase the number of local epochs? Consider both the performance and training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
