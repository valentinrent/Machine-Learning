{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning, Local Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to be imported, please make sure you have them installed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from torchvision import datasets, transforms\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we can use the GPU to accelerate the training process. We can check if a GPU is available and set the device accordingly. This will allow us to move the data and the model to the GPU. We check if a GPU is available and set the device accordingly. We also set the random seed for reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, **kwargs):\n",
    "    # original_model = models.resnet18(weights=ResNet18_Weights.DEFAULT, **kwargs)\n",
    "    original_model = models.resnet18(**kwargs)\n",
    "\n",
    "    # Replace the first convolutional layer\n",
    "    original_model.conv1 = nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "    # Remove the max pooling layer\n",
    "    original_model.maxpool = nn.Identity()\n",
    "\n",
    "    # Replace the fully connected layer\n",
    "    original_model.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    return original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_allocation(Y, num_clients):\n",
    "    client_indices = []\n",
    "\n",
    "    # Randomly shuffle indices\n",
    "    indices = np.arange(len(Y))\n",
    "    np.random.shuffle(indices, )\n",
    "    indices_split = np.array_split(indices, num_clients)\n",
    "    client_indices = [list(idx) for idx in indices_split]\n",
    "\n",
    "    return client_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_clients = 10 # Number of clients\n",
    "batch_size = 64  # Batch size for training and testing\n",
    "global_epochs = 30  # Number of global epochs\n",
    "local_epochs = 3 # Number of local epochs (round size)\n",
    "learning_rate = 1e-2  # Learning rate for the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = resnet18(10)\n",
    "client_models = [\n",
    "    deepcopy(model).to(device) for _ in range(num_clients)\n",
    "]\n",
    "client_optims = [optim.SGD(model.parameters(), lr=learning_rate) for cm in client_models]\n",
    "\n",
    "# CIFAR-10 Dataset and Dataloaders\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(), # 1) Convert images to tensors\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # 2) Normalize the dataset\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ToDo: Download the training and test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"C:/\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_subsets = uniform_allocation(\n",
    "    train_dataset.targets,\n",
    "    num_clients,\n",
    ")\n",
    "train_subsets = [\n",
    "    torch.utils.data.Subset(train_dataset, indices) for indices in train_subsets\n",
    "]\n",
    "train_subset_dataloaders = [\n",
    "    DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_subsets\n",
    "]\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"C:/\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# ToDo: Federated Learning with FedAvg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, train_loader, optimizer, loss_fn, epochs, device=\"cpu\"):\n",
    "    model = model.to(device)  # Move the model to the device\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # ToDo: Initialize the metrics\n",
    "    numclasses = len(train_loader.dataset.classes)\n",
    "    acc = Accuracy(task='multiclass', num_classes=numclasses).to(device)\n",
    "    f1 = F1Score(task='multiclass',num_classes=numclasses).to(device)\n",
    "\n",
    "    running_loss = []  # Initialize the running loss\n",
    "\n",
    "    progress_bar1 = tqdm(\n",
    "        range(epochs),\n",
    "        desc=\"Local Epochs [Loss: -,  Acc: -, AUROC: -, F1: -]\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "    )  # Progress bar for epochs\n",
    "\n",
    "    for _ in progress_bar1:\n",
    "        progress_bar2 = tqdm(\n",
    "            train_loader, desc=\"Loss: -\", position=1, leave=True\n",
    "        )  # Progress bar for batches\n",
    "\n",
    "        for x, y in progress_bar2:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ToDo: Forward pass\n",
    "            f_x = model(x)\n",
    "            loss = loss_fn(f_x, y)\n",
    "\n",
    "            # ToDo: Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # ToDo: Update the metrics\n",
    "            acc.update(f_x, y)\n",
    "            f1.update(f_x, y)\n",
    "\n",
    "            running_loss.append(loss.item())  # Update the running loss\n",
    "            progress_bar2.set_description(desc=f\"Loss: {running_loss[-1]:.3f}\")\n",
    "\n",
    "        avg_loss = sum(running_loss) / len(running_loss)\n",
    "        running_loss = []\n",
    "        progress_bar1.set_description(\n",
    "            desc=f\"Epochs [Loss: {avg_loss:.3f},  Acc: {acc.compute():.3f}, F1: {f1.compute():.3f}]\"\n",
    "        )  # Update the progress bar description for epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Decorator to disable gradient calculation\n",
    "def evaluate_global_model(model, test_loader, loss_fn, device=\"cpu\"):\n",
    "    model = model.to(device)  # Move the model to the device\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # ToDo: Initialize the metrics\n",
    "    numclasses = len(test_loader.dataset.classes)\n",
    "    acc = Accuracy(task='multiclass', num_classes=numclasses).to(device)\n",
    "    f1 = F1Score(task='multiclass',num_classes=numclasses).to(device)\n",
    "\n",
    "    running_loss = []  # Initialize the running loss\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=\"Loss: -\", leave=True)\n",
    "\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # ToDo: Forward pass\n",
    "        f_x = model(x)\n",
    "        loss = loss_fn(f_x, y)\n",
    "\n",
    "        # ToDo: Update the metrics\n",
    "        acc.update(f_x, y)\n",
    "        f1.update(f_x, y)\n",
    "\n",
    "        running_loss.append(loss.item())  # Update the running loss\n",
    "        progress_bar.set_description(desc=f\"Loss: {running_loss[-1]:.3f}\")\n",
    "\n",
    "    avg_loss = sum(running_loss) / len(running_loss)\n",
    "    print(\n",
    "        f\"Evaluation Results: [Loss: {avg_loss:.3f},  Acc: {acc.compute():.3f}, F1: {f1.compute():.3f}]\"\n",
    "    )\n",
    "\n",
    "    return acc, f1, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(\n",
    "        range(global_epochs),\n",
    "        desc=\"Epochs [Loss: -,  Acc: -, AUROC: -, F1: -]\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "    )  # Progress bar for global epochs\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_f1_scores = []\n",
    "\n",
    "#main loop\n",
    "for _ in progress_bar:\n",
    "    for model, trainset, optims in zip(client_models, train_subset_dataloaders, client_optims):\n",
    "        train_local_model(model, trainset, optims, loss_fn, local_epochs, device=\"cpu\")\n",
    "        \n",
    "    global_model = copy.deepcopy(client_models[0])  \n",
    "\n",
    "    # Iterate over each parameter in the global model\n",
    "    for global_param in global_model.parameters():\n",
    "        # Take the average of the corresponding parameters in each client model\n",
    "        global_param.data = torch.mean(torch.stack([client_param.data for client_model in client_models for client_param in client_model.parameters()]), dim=0)\n",
    "    \n",
    "    test_accuracy, test_f1, test_loss = evaluate_global_model(global_model, test_dataloader, loss_fn, device)\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_f1_scores.append(test_f1)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    #update client models\n",
    "    for client_model in client_models:\n",
    "        client_model.load_state_dict(global_model.state_dict())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show metrics\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(test_f1_scores)\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Test F1 Score\")\n",
    "plt.title(\"Test F1 Score\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(test_losses)\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"Test Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Implement the FedAVG algorithm for federated learning. We do not want a version that uses a step size, i.e., a fixed number of SGD steps before averaging the models. Instead, we want a version that uses a fixed number of communication rounds. One communication round equals a local epoch on each client. We use `local_epoch` (also referred to as \"round size\") to denote the number of local epochs. \n",
    "2. After each communication round, we want to evaluate the global model on the test set. After averaging the models, we evaluate the global model on the test set. We want to store the test loss, test accuracy, and test F1 score for each communication round and plot them at the end. \n",
    "3. Use different values for `num_clients` and `local_epoch` and compare the results. What happens if you increase the number of clients? What happens if you increase the number of local epochs? Consider both the performance and training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
